<!DOCTYPE html>
<html lang="en">

<head>

</head>

<body>
    <div th:fragment="features">
        <h2>Features - Hierarchies</h2>
        <p>
            The compositional structure of Interstakt aims to succeed in organizing music into a hierarchy of pre-allocated musical information and functions. This musical data is grouped into different levels of the composition’s hierarchy. The hierarchy provides an organizational system that allows a user to view their music from the fundamental level, as well as abstracted levels.
        </p>
        <p>
            The fundamental scenic level in Interstakt is the Gesture. Gestures are meant to contain the barebones functionality necessary in order to create musical behaviors. This will likely involve classifying how notes, velocities and durations will play out when the Gesture is activated. These parameters could be specified as a simple array of values, or a more generative function.
        </p>
        <p>
            Gesture scenes are grouped together in Phrase scenes. The Phrase provides a higher-order abstraction for Gestures, where the activation of a Phrase means that its Gestures will begin activating. Only one Gesture can play at a time within a Phrase. 
        </p>
        <p>
            There is not a specific order placed on when those Gestures are activated. They could be programmed to activate sequentially, or they could have algorithmic functions that determine different orders for their activation. Not all Gestures need to play out before a Phrase ends. So certain events can activate a new Phrase, regardless of what Gesture was currently playing in the previous Phrase. 
        </p>
        <p>
            These Phrases are further abstracted by Behavior scenes, which are the highest-order of scenic abstraction for musical behavior in Interstakt. Since Phrases are abstracted groups of fundamental Gestures, Behaviors are then the abstracted groups of these Phrase abstractions.
        </p>
        <p>
            All of these levels are contained in Voices, which correspond to the idea of an “instrument.” The musical behaviors generated by a Voice’s scenes send their information to the Voice’s delineated output. This output could be an external MIDI instrument or an MSP synthesis engine. Each Voice can only play one Gesture at a time; if a new Gesture is triggered, then all other Gestures will be deactivated.
        </p>
        <p>
            Voices are placed within an overall Score, where each plays its own material once the Score is activated. A Score contains all of the necessary functionality seen in traditional music settings. A user can control the general dynamics (tempo, amplitude, musical key) that exist in its overall orchestration of Voices. Additionally, each Voice’s current running scenic levels can be viewed within the Voice object in the Score interface. Therefore, each individual scene that is currently running in a Score can be monitored from a single interface. 
        </p>
    </div>
</body>

</html>